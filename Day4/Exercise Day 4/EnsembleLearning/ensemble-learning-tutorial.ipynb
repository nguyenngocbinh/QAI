{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a4b5a58",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "When you want to purchase a new car, will you walk up to the first car shop and purchase one based on the advice of the dealer? It’s highly unlikely.\n",
    "\n",
    "You would likely browser a few web portals where people have posted their reviews and compare different car models, checking for their features and prices. You will also probably ask your friends and colleagues for their opinion. In short, you wouldn’t directly reach a conclusion, but will instead make a decision considering the opinions of other people as well.\n",
    "\n",
    "Ensemble models in machine learning operate on a similar idea. They combine the decisions from multiple models to improve the overall performance. This can be achieved in various ways, which you will discover in this tutorial.\n",
    "\n",
    "\n",
    "## Content\n",
    "1. [Data Preprocessing](#1)                          \n",
    "1. [Modeling](#2)\n",
    "    * [Bagging meta-estimator](#3)\n",
    "    * [Random Forest](#4)\n",
    "    * [AdaBoost](#5)\n",
    "    * [Gradient Boosting](#6)\n",
    "    * [XGBoost](#7)\n",
    "    * [Stacking](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62962e06",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# 1) Data Preprocessing\n",
    "\n",
    "The dataset you are going to be using for this case study is popularly known as the Wisconsin Breast Cancer dataset. The task related to it is Classification.\n",
    "\n",
    "The dataset contains a total number of 10 features labeled in either `benign` or `malignant` classes. The features have 699 instances out of which 16 feature values are missing. The dataset only contains numeric values.\n",
    "\n",
    "You will implement the Ensembles using the mighty scikit-learn library.\n",
    "\n",
    "Let's first import all the Python dependencies you will be needing for this case study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0534abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, \\\n",
    "    GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92a6913",
   "metadata": {},
   "source": [
    "Let's load the dataset in a DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaa72fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample code number</th>\n",
       "      <th>Clump Thickness</th>\n",
       "      <th>Uniformity of Cell Size</th>\n",
       "      <th>Uniformity of Cell Shape</th>\n",
       "      <th>Marginal Adhesion</th>\n",
       "      <th>Single Epithelial Cell Size</th>\n",
       "      <th>Bare Nuclei</th>\n",
       "      <th>Bland Chromatin</th>\n",
       "      <th>Normal Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sample code number  Clump Thickness  Uniformity of Cell Size  \\\n",
       "0             1000025                5                        1   \n",
       "1             1002945                5                        4   \n",
       "2             1015425                3                        1   \n",
       "3             1016277                6                        8   \n",
       "4             1017023                4                        1   \n",
       "\n",
       "   Uniformity of Cell Shape  Marginal Adhesion  Single Epithelial Cell Size  \\\n",
       "0                         1                  1                            2   \n",
       "1                         4                  5                            7   \n",
       "2                         1                  1                            2   \n",
       "3                         8                  1                            3   \n",
       "4                         1                  3                            2   \n",
       "\n",
       "  Bare Nuclei  Bland Chromatin  Normal Nucleoli  Mitoses  Class  \n",
       "0           1                3                1        1      2  \n",
       "1          10                3                2        1      2  \n",
       "2           2                3                1        1      2  \n",
       "3           4                3                7        1      2  \n",
       "4           1                3                1        1      2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('breast_cancer_wisconsin.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8f793d",
   "metadata": {},
   "source": [
    "The column `Sample code number` is just an indicator and it's of no use in the modeling. So, let's drop it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20e5be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clump Thickness</th>\n",
       "      <th>Uniformity of Cell Size</th>\n",
       "      <th>Uniformity of Cell Shape</th>\n",
       "      <th>Marginal Adhesion</th>\n",
       "      <th>Single Epithelial Cell Size</th>\n",
       "      <th>Bare Nuclei</th>\n",
       "      <th>Bland Chromatin</th>\n",
       "      <th>Normal Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clump Thickness  Uniformity of Cell Size  Uniformity of Cell Shape  \\\n",
       "0                5                        1                         1   \n",
       "1                5                        4                         4   \n",
       "2                3                        1                         1   \n",
       "3                6                        8                         8   \n",
       "4                4                        1                         1   \n",
       "\n",
       "   Marginal Adhesion  Single Epithelial Cell Size Bare Nuclei  \\\n",
       "0                  1                            2           1   \n",
       "1                  5                            7          10   \n",
       "2                  1                            2           2   \n",
       "3                  1                            3           4   \n",
       "4                  3                            2           1   \n",
       "\n",
       "   Bland Chromatin  Normal Nucleoli  Mitoses  Class  \n",
       "0                3                1        1      2  \n",
       "1                3                2        1      2  \n",
       "2                3                1        1      2  \n",
       "3                3                7        1      2  \n",
       "4                3                1        1      2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(['Sample code number'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a835408",
   "metadata": {},
   "source": [
    "You can see that the column is dropped now. Let's get some statistics about the data using Panda's `describe()` and `info()` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e56829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clump Thickness</th>\n",
       "      <th>Uniformity of Cell Size</th>\n",
       "      <th>Uniformity of Cell Shape</th>\n",
       "      <th>Marginal Adhesion</th>\n",
       "      <th>Single Epithelial Cell Size</th>\n",
       "      <th>Bland Chromatin</th>\n",
       "      <th>Normal Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>699.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.417740</td>\n",
       "      <td>3.134478</td>\n",
       "      <td>3.207439</td>\n",
       "      <td>2.806867</td>\n",
       "      <td>3.216023</td>\n",
       "      <td>3.437768</td>\n",
       "      <td>2.866953</td>\n",
       "      <td>1.589413</td>\n",
       "      <td>2.689557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.815741</td>\n",
       "      <td>3.051459</td>\n",
       "      <td>2.971913</td>\n",
       "      <td>2.855379</td>\n",
       "      <td>2.214300</td>\n",
       "      <td>2.438364</td>\n",
       "      <td>3.053634</td>\n",
       "      <td>1.715078</td>\n",
       "      <td>0.951273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clump Thickness  Uniformity of Cell Size  Uniformity of Cell Shape  \\\n",
       "count       699.000000               699.000000                699.000000   \n",
       "mean          4.417740                 3.134478                  3.207439   \n",
       "std           2.815741                 3.051459                  2.971913   \n",
       "min           1.000000                 1.000000                  1.000000   \n",
       "25%           2.000000                 1.000000                  1.000000   \n",
       "50%           4.000000                 1.000000                  1.000000   \n",
       "75%           6.000000                 5.000000                  5.000000   \n",
       "max          10.000000                10.000000                 10.000000   \n",
       "\n",
       "       Marginal Adhesion  Single Epithelial Cell Size  Bland Chromatin  \\\n",
       "count         699.000000                   699.000000       699.000000   \n",
       "mean            2.806867                     3.216023         3.437768   \n",
       "std             2.855379                     2.214300         2.438364   \n",
       "min             1.000000                     1.000000         1.000000   \n",
       "25%             1.000000                     2.000000         2.000000   \n",
       "50%             1.000000                     2.000000         3.000000   \n",
       "75%             4.000000                     4.000000         5.000000   \n",
       "max            10.000000                    10.000000        10.000000   \n",
       "\n",
       "       Normal Nucleoli     Mitoses       Class  \n",
       "count       699.000000  699.000000  699.000000  \n",
       "mean          2.866953    1.589413    2.689557  \n",
       "std           3.053634    1.715078    0.951273  \n",
       "min           1.000000    1.000000    2.000000  \n",
       "25%           1.000000    1.000000    2.000000  \n",
       "50%           1.000000    1.000000    2.000000  \n",
       "75%           4.000000    1.000000    4.000000  \n",
       "max          10.000000   10.000000    4.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04eb22f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 699 entries, 0 to 698\n",
      "Data columns (total 10 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   Clump Thickness              699 non-null    int64 \n",
      " 1   Uniformity of Cell Size      699 non-null    int64 \n",
      " 2   Uniformity of Cell Shape     699 non-null    int64 \n",
      " 3   Marginal Adhesion            699 non-null    int64 \n",
      " 4   Single Epithelial Cell Size  699 non-null    int64 \n",
      " 5   Bare Nuclei                  699 non-null    object\n",
      " 6   Bland Chromatin              699 non-null    int64 \n",
      " 7   Normal Nucleoli              699 non-null    int64 \n",
      " 8   Mitoses                      699 non-null    int64 \n",
      " 9   Class                        699 non-null    int64 \n",
      "dtypes: int64(9), object(1)\n",
      "memory usage: 54.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea3f97",
   "metadata": {},
   "source": [
    "As mentioned earlier, the dataset contains missing values. The column named `Bare Nuclei` contains them. Let's verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e4775c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1     10\n",
       "2      2\n",
       "3      4\n",
       "4      1\n",
       "5     10\n",
       "6     10\n",
       "7      1\n",
       "8      1\n",
       "9      1\n",
       "10     1\n",
       "11     1\n",
       "12     3\n",
       "13     3\n",
       "14     9\n",
       "15     1\n",
       "16     1\n",
       "17     1\n",
       "18    10\n",
       "19     1\n",
       "20    10\n",
       "21     7\n",
       "22     1\n",
       "23     ?\n",
       "24     1\n",
       "25     7\n",
       "26     1\n",
       "27     1\n",
       "28     1\n",
       "29     1\n",
       "Name: Bare Nuclei, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Bare Nuclei'][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc33fdbf",
   "metadata": {},
   "source": [
    "You can spot some `?` in it, right? Well, these are your missing values, and you will be imputing them with Mean Imputation. But first, you will replace those `?` with `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d1c5005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1     10\n",
       "2      2\n",
       "3      4\n",
       "4      1\n",
       "5     10\n",
       "6     10\n",
       "7      1\n",
       "8      1\n",
       "9      1\n",
       "10     1\n",
       "11     1\n",
       "12     3\n",
       "13     3\n",
       "14     9\n",
       "15     1\n",
       "16     1\n",
       "17     1\n",
       "18    10\n",
       "19     1\n",
       "20    10\n",
       "21     7\n",
       "22     1\n",
       "23     0\n",
       "24     1\n",
       "25     7\n",
       "26     1\n",
       "27     1\n",
       "28     1\n",
       "29     1\n",
       "Name: Bare Nuclei, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.replace('?', 0, inplace=True)\n",
    "data['Bare Nuclei'][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406916d8",
   "metadata": {},
   "source": [
    "The `?` are replaced with `0` now. Let's do the missing value treatment now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a67a352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  1.,  1., ...,  1.,  1.,  2.],\n",
       "       [ 5.,  4.,  4., ...,  2.,  1.,  2.],\n",
       "       [ 3.,  1.,  1., ...,  1.,  1.,  2.],\n",
       "       ...,\n",
       "       [ 5., 10., 10., ..., 10.,  2.,  4.],\n",
       "       [ 4.,  8.,  6., ...,  6.,  1.,  4.],\n",
       "       [ 4.,  8.,  8., ...,  4.,  1.,  4.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer = SimpleImputer()\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a379d048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     699\n",
       "unique     11\n",
       "top         1\n",
       "freq      402\n",
       "Name: Bare Nuclei, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data['Bare Nuclei']\n",
    "import numpy as np\n",
    "x.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af56906",
   "metadata": {},
   "source": [
    "Now if you take a look at the dataset itself, you will see that all the ranges of the features of the dataset are not the same. This may cause a problem. A small change in a feature might not affect the other. To address this problem, you will normalize the ranges of the features to a uniform range, in this case, 0 - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d07b28a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44444444, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.44444444, 0.33333333, 0.33333333, ..., 0.11111111, 0.        ,\n",
       "        0.        ],\n",
       "       [0.22222222, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.44444444, 1.        , 1.        , ..., 1.        , 0.11111111,\n",
       "        1.        ],\n",
       "       [0.33333333, 0.77777778, 0.55555556, ..., 0.55555556, 0.        ,\n",
       "        1.        ],\n",
       "       [0.33333333, 0.77777778, 0.77777778, ..., 0.33333333, 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "normalized_data = scaler.fit_transform(imputed_data)\n",
    "normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f45516",
   "metadata": {},
   "source": [
    "Wonderful!\n",
    "\n",
    "You have performed all the preprocessing that was required in order to perform your Ensembling experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb13264",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# 2) Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5362835",
   "metadata": {},
   "source": [
    "Separated data as train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63627996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segregate the features from the labels\n",
    "X = normalized_data[:, 0:9]\n",
    "Y = normalized_data[:, 9]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c89b52a",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## Bagging meta-estimator\n",
    "\n",
    "Bagging meta-estimator is an ensembling algorithm that can be used for both classification (BaggingClassifier) and regression (BaggingRegressor) problems. It follows the typical bagging technique to make predictions. Following are the steps for the bagging meta-estimator algorithm:\n",
    "\n",
    "1. Random subsets are created from the original dataset (Bootstrapping).\n",
    "2. The subset of the dataset includes all features.\n",
    "3. A user-specified base estimator is fitted on each of these smaller sets.\n",
    "4. Predictions from each model are combined to get the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d14d43c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9714285714285714\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[139   4]\n",
      " [  2  65]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.97      0.98       143\n",
      "         1.0       0.94      0.97      0.96        67\n",
      "\n",
      "    accuracy                           0.97       210\n",
      "   macro avg       0.96      0.97      0.97       210\n",
      "weighted avg       0.97      0.97      0.97       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight='balanced', random_state=2),\n",
    "                          random_state=2)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# Check accuracy\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08b8b20",
   "metadata": {},
   "source": [
    "For the regression problem, replace `BaggingClassifier` with `BaggingRegressor`.\n",
    "\n",
    "Parameters used in the  algorithms:\n",
    "\n",
    "* <b>base_estimator</b>:\n",
    "    * It defines the base estimator to fit on random subsets of the dataset.\n",
    "    * When nothing is specified, the base estimator is a decision tree.\n",
    "* <b>n_estimators</b>:\n",
    "    * It is the number of base estimators to be created.\n",
    "    * The number of estimators should be carefully tuned as a large number would take a very long time to run, while a very small number might not provide the best results.\n",
    "* <b>max_samples</b>:\n",
    "    * This parameter controls the size of the subsets.\n",
    "    * It is the maximum number of samples to train each base estimator.\n",
    "* <b>max_features</b>:\n",
    "    * Controls the number of features to draw from the whole dataset.\n",
    "    * It defines the maximum number of features required to train each base estimator.\n",
    "* <b>n_jobs</b>:\n",
    "    * The number of jobs to run in parallel.\n",
    "    * Set this value equal to the cores in your system.\n",
    "    * If -1, the number of jobs is set to the number of cores.\n",
    "* <b>random_state</b>:\n",
    "    * It specifies the method of random split. When random state value is same for two models, the random selection is same for both models.\n",
    "    * This parameter is useful when you want to compare different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058c9ae4",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## Random Forest\n",
    "\n",
    "Random Forest is another ensemble machine learning algorithm that follows the bagging technique. It is an extension of the bagging estimator algorithm. The base estimators in random forest are decision trees. Unlike bagging meta estimator, random forest randomly selects a set of features which are used to decide the best split at each node of the decision tree.\n",
    "\n",
    "Looking at it step-by-step, this is what a random forest model does:\n",
    "\n",
    "1. Random subsets are created from the original dataset (bootstrapping).\n",
    "2. At each node in the decision tree, only a random set of features are considered to decide the best split.\n",
    "3. A decision tree model is fitted on each of the subsets.\n",
    "4. The final prediction is calculated by averaging the predictions from all decision trees.\n",
    "\n",
    "<i>Note: The decision trees in random forest can be built on a subset of data and features. Particularly, the sklearn model of random forest uses all features for decision tree and a subset of features are randomly selected for splitting at each node.</i>\n",
    "\n",
    "To sum up, Random forest <b>randomly</b> selects data points and features, and builds <b>multiple trees (Forest)</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72759b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9761904761904762\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[140   3]\n",
      " [  2  65]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.98       143\n",
      "         1.0       0.96      0.97      0.96        67\n",
      "\n",
      "    accuracy                           0.98       210\n",
      "   macro avg       0.97      0.97      0.97       210\n",
      "weighted avg       0.98      0.98      0.98       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(class_weight='balanced', random_state=2)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# Check accuracy\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a766e843",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "\n",
    "* <b>n_estimators</b>:\n",
    "    * It defines the number of decision trees to be created in a random forest.\n",
    "    * Generally, a higher number makes the predictions stronger and more stable, but a very large number can result in higher training time.\n",
    "* <b>criterion</b>:\n",
    "    * It defines the function that is to be used for splitting.\n",
    "    * The function measures the quality of a split for each feature and chooses the best split.\n",
    "* <b>max_features</b>:\n",
    "    * It defines the maximum number of features allowed for the split in each decision tree.\n",
    "    * Increasing max features usually improve performance but a very high number can decrease the diversity of each tree.\n",
    "* <b>max_depth</b>:\n",
    "    * Random forest has multiple decision trees. This parameter defines the maximum depth of the trees.\n",
    "* <b>min_samples_split</b>:\n",
    "    * Used to define the minimum number of samples required in a leaf node before a split is attempted.\n",
    "    * If the number of samples is less than the required number, the node is not split.\n",
    "* <b>min_samples_leaf</b>:\n",
    "    * This defines the minimum number of samples required to be at a leaf node.\n",
    "    * Smaller leaf size makes the model more prone to capturing noise in train data.\n",
    "* <b>max_leaf_nodes</b>:\n",
    "    * This parameter specifies the maximum number of leaf nodes for each tree.\n",
    "    * The tree stops splitting when the number of leaf nodes becomes equal to the max leaf node.\n",
    "* <b>n_jobs</b>:\n",
    "    * This indicates the number of jobs to run in parallel.\n",
    "    * Set value to -1 if you want it to run on all cores in the system.\n",
    "* <b>random_state</b>:\n",
    "    * This parameter is used to define the random selection.\n",
    "    * It is used for comparison between various models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcd8032",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## AdaBoost\n",
    "\n",
    "Adaptive boosting or AdaBoost is one of the simplest boosting algorithms. Usually, decision trees are used for modelling. Multiple sequential models are created, each correcting the errors from the last model. AdaBoost assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly.\n",
    "\n",
    "Below are the steps for performing the AdaBoost algorithm:\n",
    "\n",
    "1. Initially, all observations in the dataset are given equal weights.\n",
    "2. A model is built on a subset of data.\n",
    "3. Using this model, predictions are made on the whole dataset.\n",
    "4. Errors are calculated by comparing the predictions and actual values.\n",
    "5. While creating the next model, higher weights are given to the data points which were predicted incorrectly.\n",
    "6. Weights can be determined using the error value. For instance, higher the error more is the weight assigned to the observation.\n",
    "7. This process is repeated until the error function does not change, or the maximum limit of the number of estimators is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "762057f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9333333333333333\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[138   5]\n",
      " [  9  58]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.97      0.95       143\n",
      "         1.0       0.92      0.87      0.89        67\n",
      "\n",
      "    accuracy                           0.93       210\n",
      "   macro avg       0.93      0.92      0.92       210\n",
      "weighted avg       0.93      0.93      0.93       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(class_weight='balanced', random_state=2),\n",
    "                           random_state=2)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# Check accuracy\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56452ea",
   "metadata": {},
   "source": [
    "For the regression problem, replace `AdaBoostClassifier` with `AdaBoostRegressor`.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "* <b>base_estimators</b>:\n",
    "    * It helps to specify the type of base estimator, that is, the machine learning algorithm to be used as base learner.\n",
    "* <b>n_estimators</b>:\n",
    "    * It defines the number of base estimators.\n",
    "    * The default value is 10, but you should keep a higher value to get better performance.\n",
    "* <b>learning_rate</b>:\n",
    "    * This parameter controls the contribution of the estimators in the final combination.\n",
    "    * There is a trade-off between learning_rate and n_estimators.\n",
    "* <b>max_depth</b>:\n",
    "    * Defines the maximum depth of the individual estimator.\n",
    "    * Tune this parameter for best performance.\n",
    "* <b>n_jobs</b>:\n",
    "    * Specifies the number of processors it is allowed to use.\n",
    "    * Set value to -1 for maximum processors allowed.\n",
    "* <b>random_state</b>:\n",
    "    * An integer value to specify the random data split.\n",
    "    * A definite value of random_state will always produce same results if given with same parameters and training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d81d22",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## Gradient Boosting\n",
    "\n",
    "Gradient Boosting or GBM is another ensemble machine learning algorithm that works for both regression and classification problems. GBM uses the boosting technique, combining a number of weak learners to form a strong learner. Regression trees used as a base learner, each subsequent tree in series is built on the errors calculated by the previous tree.\n",
    "\n",
    "We will use a simple example to understand the GBM algorithm. We have to predict the age of a group of people using the below data:\n",
    "\n",
    "| ID | Married | Gender | City | Monthly Income | Age (target) |\n",
    "| :- | :- | :- | :- | :- | :- |\n",
    "| 1 | Y\t| F\t| Hanoi\t| 51.000 | 35 |\n",
    "| 2 | N\t| M\t| HCM | 25.000 | 24 |\n",
    "| 3\t| Y\t| F\t| Hanoi | 70.000 | 38 |\n",
    "| 4\t| Y\t| M\t| HCM | 53.000 | 30 |\n",
    "| 5\t| N\t| M\t| Hanoi\t| 47.000 | 33 |\n",
    "\n",
    "1. Train the first model on the above dataset.\n",
    "2. Calculate the error based on the error between the actual value and the predicted value.\n",
    "\n",
    "| ID | Married | Gender | City | Monthly Income | Age (target) | Age (prediction 1) | Error 1 |\n",
    "| :- | :- | :- | :- | :- | :- | :- | :- |\n",
    "| 1 | Y\t| F\t| Hanoi | 51.000 | 35 | 32 | 3 |\n",
    "| 2\t| N\t| M\t| HCM | 25.000 | 24 | 32 | -8 |\n",
    "| 3\t| Y\t| F\t| Hanoi | 70.000 | 38 | 32 | 6 |\n",
    "| 4\t| Y\t| M\t| HCM | 53.000 | 30 | 32 | -2 |\n",
    "| 5\t| N\t| M\t| Hanoi | 47.000 | 33 | 32 | 1 |\n",
    "\n",
    "3. A second model is created, using the same input features as the previous model, but the target is `Error 1`.\n",
    "4. The predicted value of the second model is added to the predicted value of the first model.\n",
    "\n",
    "| ID | Age (target) | Age (prediction 1) | Error 1 (new target) | Prediction 2 | Combine (Pred1+Pred2) |\n",
    "| :- | :- | :- | :- | :- | :- |\n",
    "| 1\t| 35 | 32 | 3 | 3 | 35 |\n",
    "| 2\t| 24 | 32 | -8 | -5 | 27 |\n",
    "| 3\t| 38 | 32 | 6 | 3 | 35 |\n",
    "| 4\t| 30 | 32 | -2 | -5 | 27 |\n",
    "| 5\t| 33 | 32 | 1 | 3 | 35 |\n",
    "\n",
    "5. The combined value in step 3 is considered as the new predicted value. We calculate the error (`Error 2`) based on the error between this value and the actual value.\n",
    "\n",
    "| ID | Age (target) | Age (prediction 1) | Error 1 (new target) | Prediction 2 | Combine (Pred1+Pred2) | Error 2 |\n",
    "| :- | :- | :- | :- | :- | :- | :- |\n",
    "| 1\t| 35 | 32 | 3 | 3 | 35 | 0 |\n",
    "| 2\t| 24 | 32 | -8 | -5 | 27 | -3 |\n",
    "| 3\t| 38 | 32 | 6 | 3 | 35 | 3 |\n",
    "| 4\t| 30 | 32 | -2 | -5 | 27 | 3 |\n",
    "| 5\t| 33 | 32 | 1 | 3 | 35 | -3 |\n",
    "\n",
    "6. Steps 2 to 5 are repeated till the maximum number of iterations is reached (or error function does not change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae2d7572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9571428571428572\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[139   4]\n",
      " [  5  62]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.97      0.97       143\n",
      "         1.0       0.94      0.93      0.93        67\n",
      "\n",
      "    accuracy                           0.96       210\n",
      "   macro avg       0.95      0.95      0.95       210\n",
      "weighted avg       0.96      0.96      0.96       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(learning_rate=0.01, random_state=2)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# Check accuracy\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9932d0",
   "metadata": {},
   "source": [
    "For the regression problem, replace `GradientBoostingClassifier` with `GradientBoostingRegressor`.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "* <b>min_samples_split</b>:\n",
    "    * Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting.\n",
    "    * Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "* <b>min_samples_leaf</b>:\n",
    "    * Defines the minimum samples required in a terminal or leaf node.\n",
    "    * Generally, lower values should be chosen for imbalanced class problems because the regions in which the minority class will be in the majority will be very small.\n",
    "* <b>min_weight_fraction_leaf</b>:\n",
    "    * Similar to min_samples_leaf but defined as a fraction of the total number of observations instead of an integer.\n",
    "* <b>max_depth</b>:\n",
    "    * The maximum depth of a tree.\n",
    "    * Used to control over-fitting as higher depth will allow the model to learn relations very specific to a particular sample.\n",
    "    * Should be tuned using CV.\n",
    "* <b>max_leaf_nodes</b>:\n",
    "    * The maximum number of terminal nodes or leaves in a tree.\n",
    "    * Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    * If this is defined, GBM will ignore max_depth.\n",
    "* <b>max_features</b>\n",
    "    * The number of features to consider while searching for the best split. These will be randomly selected.\n",
    "    * As a thumb-rule, the square root of the total number of features works great but we should check up to 30-40% of the total number of features.\n",
    "    * Higher values can lead to over-fitting but it generally depends on a case to case scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95c9cdd",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "## XGBoost\n",
    "\n",
    "XGBoost (extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm. XGBoost has proved to be a highly effective ML algorithm, extensively used in machine learning competitions and hackathons. XGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques. It also includes a variety of regularization which reduces overfitting and improves overall performance. Hence it is also known as <b>regularized boosting</b> technique.\n",
    "\n",
    "Let us see how XGBoost is comparatively better than other techniques:\n",
    "\n",
    "* <b>Regularization</b>:\n",
    "    * Standard GBM implementation has no regularisation like XGBoost.\n",
    "    * Thus XGBoost also helps to reduce overfitting.\n",
    "* <b>Parallel Processing</b>:\n",
    "    * XGBoost implements parallel processing and is faster than GBM .\n",
    "    * XGBoost also supports implementation on Hadoop.\n",
    "* <b>High Flexibility</b>:\n",
    "    * XGBoost allows users to define custom optimization objectives and evaluation criteria adding a whole new dimension to the model.\n",
    "* <b>Handling Missing Values</b>:\n",
    "    * XGBoost has an in-built routine to handle missing values.\n",
    "* <b>Tree Pruning</b>:\n",
    "    * XGBoost makes splits up to the max_depth specified and then starts pruning the tree backwards and removes splits beyond which there is no positive gain.\n",
    "* <b>Built-in Cross-Validation</b>:\n",
    "    * XGBoost allows a user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.\n",
    "    \n",
    "Since XGBoost takes care of the missing values itself, you do not have to impute the missing values. You can skip the step for missing value imputation from the code mentioned above. Follow the remaining steps as always and then apply xgboost as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2c8f4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9571428571428572\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[138   5]\n",
      " [  4  63]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.97      0.97       143\n",
      "         1.0       0.93      0.94      0.93        67\n",
      "\n",
      "    accuracy                           0.96       210\n",
      "   macro avg       0.95      0.95      0.95       210\n",
      "weighted avg       0.96      0.96      0.96       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(random_state=2, eta=0.01)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# Check accuracy\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f103caee",
   "metadata": {},
   "source": [
    "For the regression problem, replace `XGBClassifier` with `XGBRegressor`.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "* <b>nthread</b>:\n",
    "    * This is used for parallel processing and the number of cores in the system should be entered..\n",
    "    * If you wish to run on all cores, do not input this value. The algorithm will detect it automatically.\n",
    "* <b>eta</b>:\n",
    "    * Analogous to learning rate in GBM.\n",
    "    * Makes the model more robust by shrinking the weights on each step.\n",
    "* <b>min_child_weight</b>:\n",
    "    * Defines the minimum sum of weights of all observations required in a child.\n",
    "    * Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "* <b>max_depth</b>:\n",
    "    * It is used to define the maximum depth.\n",
    "    * Higher depth will allow the model to learn relations very specific to a particular sample.\n",
    "* <b>max_leaf_nodes</b>:\n",
    "    * The maximum number of terminal nodes or leaves in a tree.\n",
    "    * Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    * If this is defined, GBM will ignore max_depth.\n",
    "* <b>gamma</b>:\n",
    "    * A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    * Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "* <b>subsample</b>:\n",
    "    * Same as the subsample of GBM. Denotes the fraction of observations to be randomly sampled for each tree.\n",
    "    * Lower values make the algorithm more conservative and prevent overfitting but values that are too small might lead to under-fitting.\n",
    "* <b>colsample_bytree</b>:\n",
    "    * It is similar to max_features in GBM.\n",
    "    * Denotes the fraction of columns to be randomly sampled for each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a8d5c1",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "## Stacking\n",
    "\n",
    "Stacking is an ensemble learning technique that uses predictions from multiple models (for example decision tree, knn or svm) to build a new model. This model is used for making predictions on the test set. Below is a step-wise explanation for a simple stacked ensemble:\n",
    "\n",
    "1. The train set is split into 10 parts.\n",
    "<img src=\"stack1.png\" width=200px>\n",
    "2. A base model (suppose a decision tree) is fitted on 9 parts and predictions are made for the 10th part. This is done for each part of the train set.\n",
    "<img src=\"stack2.png\" width=250px>\n",
    "3. The base model (in this case, decision tree) is then fitted on the whole train dataset.\n",
    "4. Using this model, predictions are made on the test set.\n",
    "<img src=\"stack3.png\" width=250px>\n",
    "5. Steps 2 to 4 are repeated for another base model (say knn) resulting in another set of predictions for the train set and test set.\n",
    "<img src=\"stack4.png\" width=280px>\n",
    "6. The predictions from the train set are used as features to build a new model.\n",
    "<img src=\"stack5.png\" width=200px>\n",
    "7. This model is used to make final predictions on the test prediction set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "017ffcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9809523809523809\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[141   2]\n",
      " [  2  65]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       143\n",
      "         1.0       0.97      0.97      0.97        67\n",
      "\n",
      "    accuracy                           0.98       210\n",
      "   macro avg       0.98      0.98      0.98       210\n",
      "weighted avg       0.98      0.98      0.98       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "estimators = [\n",
    "    ('lr', LogisticRegression(class_weight='balanced', random_state=2)),\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=2)),\n",
    "    ('svm', SVC(class_weight='balanced', random_state=2))\n",
    "]\n",
    "final_estimator = LogisticRegression(class_weight='balanced', random_state=2)\n",
    "model = StackingClassifier(estimators=estimators, final_estimator=final_estimator, cv=5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# Check accuracy\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc45966",
   "metadata": {},
   "source": [
    "For the regression problem, replace `StackingClassifier` with `StackingRegressor`.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "* <b>estimators</b>:\n",
    "    * Base estimators which will be stacked together.\n",
    "    * Each element of the list is defined as a tuple of string (i.e. name) and an estimator instance.\n",
    "* <b>final_estimator</b>:\n",
    "    * A classifier which will be used to combine the base estimators.\n",
    "    * The default classifier is LogisticRegression.\n",
    "* <b>cv</b>:\n",
    "    * Determines the cross-validation splitting strategy used in `cross_val_predict` to train `final_estimator`.\n",
    "    * Possible inputs for cv are:\n",
    "        * None, to use the default 5-fold cross validation,\n",
    "        * integer, to specify the number of folds in a (Stratified) KFold,\n",
    "        * An object to be used as a cross-validation generator,\n",
    "        * An iterable yielding train, test splits.\n",
    "* <b>stack_method</b>:\n",
    "    * Methods called for each base estimator.\n",
    "* <b>n_jobs</b>:\n",
    "    * The number of jobs to run in parallel all `estimators` `fit`.\n",
    "* <b>passthrough</b>:\n",
    "    * When False, only the predictions of estimators will be used as training data for `final_estimator`.\n",
    "    * When True, the `final_estimator` is trained on the predictions as well as the original training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
