---
title: An R Markdown document converted from "exercise_1_housing_price_prediction/Linear
  Regression.ipynb"
output: html_document
---

## Housing Price Prediction

### Problem Statement 
A real estate company has a dataset containing the price of properties in Delhi region. They want our company to use thier dataset and can predict the house price base on the import factors such as area, bedrooms, parking,...

### To Do List
* Indentify the important feature which are affecting the house price
* Create and Train the Machine Learning Model to predict the house price
* Evaluate the accuracy of the model

```{python}
import pandas as pd
```

## Step I. Read and understand the dataset

First step, we will use the pandas library to read the dataset

```{python}
housedf = pd.read_csv("exercise_1_housing_price_prediction/data/Housing.csv")
```

```{python}
housedf.head()
```

```{python}
housedf.shape
```

We had the dataset had 545 records and 13 feature:
1. price: the house price
2. area: the area of the house 
3. bedrooms: number of the bedrooms
4. bathrooms: number of the bathrooms
5. stories: number of floors of the house
6. mainroad: are the house near main road
7. guestroom: had guest house of not
8. basement: had basement of not
9. hotwaterheating: had hotwater heating system or not
10. airconditioning: had air conditioning system or not
11. parking: the number of the parking
12. prefarea: had yard or not
13. furnishingstatus: furnishing status

After see this, we can determined that the target feature is price and the feature is area, bedrooms, bathrooms, ...

```{python}
housedf.describe()
```

## Step II: Data Cleaning

Next we will go to data cleaning step

### What is data cleaning?

    Data cleaning is the process if fixing or remove incorrect, error, wrong format, duplicate in the dataset.

### Why we need data cleaning?

    When we combining multiple data sources, our datasets will had many data which are incorrect, error, duplicate, ... 
    If we don't have data cleaning, the data is incorrect and the outcome and algorithm will be unreliable.

### Five characteristics of quality data: <br>

* Validity: The degree to which your data conforms to defined business rules or constraints.
* Accuracy: Ensure your data is close to the true values.
* Completeness: The degree to which all required data is known.
* Consistency: Ensure your data is consistent within the same dataset and/or across multiple data sets.
* Uniformity: The degree to which the data is specified using the same unit of measure.

### How do you clean data?<br>
* Step 1: Remove duplicate or irrelevant observations
* Step 2: Fix structural errors
* Step 3: Filter unwanted outliers
* Step 4: Handle missing data
* Step 5: Validate and QA

### Let coding

#### Step 1: We will remove the records which are duplicate in our dataset

```{python}
housedf = housedf.drop_duplicates()
housedf.shape
```

We can see the dataset shape is not change. It means there are no duplicate records in our dataset.

#### Step 2: Fix structural errors.

```{python}
housedf.info()
```

We will check the name of columns and the data type of each column.

#### Step 3: Filter outliers

# ![IQR.png](attachment:IQR.png)

For filtering outliers, we will use the IQR method. The IQR is the interquartile range. The interquartile range is the range of the 25th percentiles (Q1) and 75th percentiles (Q3). <br>
* Step 1: Calculate the first quartile (Q1) and third quartile (Q3) of the dataset.
* Step 2: Calculate the interquartile range (IQR) as Q3 - Q1.
* Step 3: Calculate the upper and lower limits of the outliers as Q3 + 1.5 * IQR and Q1 - 1.5 * IQR.
* Step 4: Remove the outliers from the dataset.

Now, I will ourlier analysis with 4 feature is: price, bedrooms, bathroom and stories using matplotlib and seaborn library

```{python}
import matplotlib.pyplot as plt
import seaborn as sns
```

```{python}
fig, axs = plt.subplots(2, 2, figsize=(10, 5))
plt1 = sns.boxplot(housedf['price'], ax=axs[0, 0])
plt3 = sns.boxplot(housedf['bedrooms'], ax=axs[0, 1])
plt1 = sns.boxplot(housedf['bathrooms'], ax=axs[1, 0])
plt2 = sns.boxplot(housedf['stories'], ax=axs[1, 1])
plt.tight_layout()
```

```{python}
#TO DO: From the example above, perform outlier analysis for 2 features: area and parking features
# sns.boxplot(housedf['area'])
# sns.boxplot(housedf['parking'])
fig2, axs = plt.subplots(1, 2, figsize=(10, 5))
plt1 = sns.boxplot(housedf['area'], ax=axs[0])
plt3 = sns.boxplot(housedf['parking'], ax=axs[1])
plt.tight_layout()
```

We can see the Price feature and [............] feature have considerable outliers.

Remember: Just because an outlier exists, doesn’t mean it is incorrect.

Now, we will remove the outliers from the dataset in Price feature and [............] feature.

```{python}
# The price before filtering the outliers
plt.boxplot(housedf.price)
```

```{python}
# Outlier treatment for price and visualize the distribution of the price after filtering the outliers
Q1 = housedf.price.quantile(0.25)
Q3 = housedf.price.quantile(0.75)
IQR = Q3 - Q1
housedf = housedf[(housedf.price >= Q1 - 1.5*IQR) &(housedf.price <= Q3 + 1.5*IQR)]
plt.boxplot(housedf.price)
```

```{python}
#TO DO: Visualize the distribution of the [.......] before filtering the outliers
```

```{python}
#TO DO: Outlier treatment for [........] and visualize the distribution of the [.........] after filtering the outliers
```

```{python}
# Outlier Analysis
fig, axs = plt.subplots(2, 3, figsize=(10, 5))
plt1 = sns.boxplot(housedf['price'], ax=axs[0, 0])
plt2 = sns.boxplot(housedf['area'], ax=axs[0, 1])
plt3 = sns.boxplot(housedf['bedrooms'], ax=axs[0, 2])
plt1 = sns.boxplot(housedf['bathrooms'], ax=axs[1, 0])
plt2 = sns.boxplot(housedf['stories'], ax=axs[1, 1])
plt3 = sns.boxplot(housedf['parking'], ax=axs[1, 2])
plt.tight_layout()
```

#### Step 4: Handle missing data

We will check and find the missing data in the dataset.

```{python}
housedf.isnull().sum()*100/housedf.shape[0]
```

We can see there are no NULL values in the dataset, the dataset it is clean.

#### Step 5: Validate and QA

## Step III: Exploratory Data Analytics (EDA)

Let's now spend some time doing what is arguably the most important step - understanding the data.

If there is some obvious multicollinearity going on, this is the first place to catch it
Here's where you'll also identify if some predictors directly have a strong association with the outcome variable

### Visualising Numeric Variables

Let's make a pairplot of all the numeric variables

```{python}
sns.pairplot(housedf)
plt.show()
```

### Visualising Categorical Variables

As you might have noticed, there are a few categorical variables as well. Let's make a boxplot for some of these variables.

```{python}
plt.figure(figsize=(20, 12))
plt.subplot(2,3,1)
sns.boxplot(x = 'mainroad', y = 'price', data = housedf)
plt.subplot(2,3,2)
sns.boxplot(x = 'guestroom', y = 'price', data = housedf)
plt.subplot(2,3,3)
sns.boxplot(x = 'basement', y = 'price', data = housedf)
plt.show()
```

```{python}
# TO DO: EDA 3 categorical variables: hotwaterheating, airconditioning and furnishingstatus|
```

We can also visualise some of these categorical features parallely by using the hue argument. Below is the plot for furnishingstatus with airconditioning as the hue.

```{python}
plt.figure(figsize=(10, 5))
sns.boxplot(x='furnishingstatus', y='price', hue='airconditioning', data=housedf)
plt.show()
```

```{python}
# TO DO: Try to visualize the relationship between some features and the price
```

## Step IV: Data Preparation

* You can see that your dataset has many columns with values as 'Yes' or 'No'.

* But in order to fit a regression line, we would need numerical values and not string. Hence, we need to convert them to 1s and 0s, where 1 is a 'Yes' and 0 is a 'No'.

```{python}
# List of variables to map

varlist =  ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']

# Defining the map function
def binary_map(x):
    return x.map({'yes': 1, "no": 0})

# Applying the function to the housing list
housedf[varlist] = housedf[varlist].apply(binary_map)

# Check the housing dataframe now
housedf.head()
```

### Dummy Variables

The variable furnishingstatus has three levels. We need to convert these levels into integer as well.

For this, we will use something called dummy variables.

```{python}
# Get the dummy variables for the feature 'furnishingstatus' and store it in a new variable - 'status'
status = pd.get_dummies(housedf['furnishingstatus'])

# Check what the dataset 'status' looks like
status.head()
```

Now, you don't need three columns. You can drop the furnished column, as the type of furnishing can be identified with just the last two columns where —

* 00 will correspond to furnished
* 01 will correspond to unfurnished
* 10 will correspond to semi-furnished

```{python}
# Add the results to the original housing dataframe

housedf = pd.concat([housedf, status], axis = 1)

# Now let's see the head of our dataframe.

housedf.head()
```

```{python}
# Drop 'furnishingstatus' as we have created the dummies for it

housedf.drop(['furnishingstatus'], axis=1, inplace=True)

housedf.head()
```

### Splitting the Data into Training and Testing Sets

```{python}
import numpy as np
import sklearn
```

```{python}
from sklearn.model_selection import train_test_split

# We specify this so that the train and test data set always have the same rows, respectively
np.random.seed(42)
df_train, df_test = train_test_split(housedf, train_size = 0.7, test_size = 0.3, random_state = 100)
```

### Rescaling the Features
As you saw in the demonstration for Simple Linear Regression, scaling doesn't impact your model. Here we can see that except for area, all the columns have small integer values. So it is extremely important to rescale the variables so that they have a comparable scale. If we don't have comparable scales, then some of the coefficients as obtained by fitting the regression model might be very large or very small as compared to the other coefficients. This might become very annoying at the time of model evaluation. So it is advised to use standardization or normalization so that the units of the coefficients obtained are all on the same scale. As you know, there are two common ways of rescaling:

1. Min-Max scaling
2. Standardisation (mean-0, sigma-1) <br>
    x = X - mean(X) / sigma(X)
    
This time, we will use MinMax scaling.

```{python}
from sklearn.preprocessing import MinMaxScaler
```

```{python}
scaler = MinMaxScaler()
```

```{python}
# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables
num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking', 'price']

df_train[num_vars] = scaler.fit_transform(df_train[num_vars])
```

```{python}
df_train.head()
```

```{python}
df_train.describe()
```

```{python}
# Let's check the correlation coefficients to see which variables are highly correlated

plt.figure(figsize=(16, 10))
sns.heatmap(df_train.corr(), annot=True, cmap="YlGnBu")
plt.show()
```

As you might have noticed, area seems to the correlated to price the most. Let's see a pairplot for area vs price.

Dividing into X and Y sets for the model building

```{python}
y_train = df_train.pop('price')
X_train = df_train
```

### Model Building

This time, we will be using the LinearRegression function from SciKit Learn for its compatibility with RFE (which is a utility from sklearn)

### RFE

Recursive feature elimination

```{python}
# Importing RFE and LinearRegression
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
```

```{python}
lm = LinearRegression()
lm.fit(X_train, y_train)
```

```{python}
rfe = RFE(lm)
rfe = rfe.fit(X_train, y_train)
```

```{python}
list(zip(X_train.columns, rfe.support_, rfe.ranking_))
```

```{python}
col = X_train.columns[rfe.support_]
col
```

```{python}
X_train.columns[~rfe.support_]
```

### Building model and visualization for 2 features

```{python}
# Filter the columns with 2 features is price and area
X_train_ex1 = X_train['area'].values.reshape(-1, 1)
```

```{python}
# Create new model linear regression
model_1 = LinearRegression()

# Fit dataset
model_1.fit(X_train_ex1, y_train)

# Predict
predict_1 = model_1.predict(X_train_ex1)

# Evaluate  
r2 = model_1.score(X_train_ex1, y_train)
print("R2 score: " + str(r2))
```

```{python}
# Plot the result 
plt.style.use('default')
plt.style.use('ggplot')

fig, ax = plt.subplots(figsize=(8, 4))

ax.plot(X_train_ex1, predict_1, color='k', label='Regression model')
ax.scatter(X_train_ex1, y_train, edgecolor='k', facecolor='grey',
           alpha=0.7, label='Sample data')
ax.set_ylabel('House Price', fontsize=14)
ax.set_xlabel('Area', fontsize=14)
ax.legend(facecolor='white', fontsize=11)
ax.set_title('$R^2= %.2f$' % r2, fontsize=18)

fig.tight_layout()
```

```{python}
# TO DO: Build your own linear regression model with feature price and 1 more other feature.
```

### Building model using statsmodel, for the detailed statistics

```{python}
# Creating X_test dataframe with RFE selected variables
X_train_rfe = X_train[col]
```

```{python}
# Adding a constant variable
import statsmodels.api as sm
X_train_rfe = sm.add_constant(X_train_rfe)
```

```{python}
lm = sm.OLS(y_train, X_train_rfe).fit()
```

```{python}
#Let's see the summary of our linear model
print(lm.summary())
```

### Predicting the test data set

```{python}
y_test = df_test.pop('price')
X_test = df_test
```

```{python}
# Adding constant variable to test dataframe
X_test = sm.add_constant(X_test)
```

```{python}
# Creating X_test_new dataframe by dropping variables from X_test
X_test_rfe = X_test[X_train_rfe.columns]
```

```{python}
# Making predictions
y_pred = lm.predict(X_test_rfe)
```

```{python}
from sklearn.metrics import r2_score
r2_score(y_test, y_pred)
```

```{python}
# Plotting y_test and y_pred to understand the spread.
fig = plt.figure()
plt.scatter(y_test, y_pred)
fig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading
plt.xlabel('y_test', fontsize=18)                          # X-label
plt.ylabel('y_pred', fontsize=16)                          # Y-label
```

We can see that the equation of our best fitted line is:

price  = [...] × area + [...] × bathrooms + [...] × stories + [...] × airconditioning + [...] × parking + [...] × prefarea

